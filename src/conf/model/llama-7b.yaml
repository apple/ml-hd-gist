# @package _global_
# Hyperparams from https://github.com/tatsu-lab/stanford_alpaca#fine-tuning

model:
  model_name_or_path: baffo32/decapoda-research-llama-7B-hf
#  model_name_or_path: llama-7b

training:
  bf16: true
  bf16_full_eval: true
  # 128 batch size.
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_checkpointing: false
  gradient_accumulation_steps: 4
  generation_max_length: 1024  # This includes the prompt length.
  learning_rate: 2.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  num_train_epochs: 1
  save_steps: 1000000
  eval_steps: 1000000
